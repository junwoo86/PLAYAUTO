import pandas as pd
import numpy as np
import pickle
from datetime import datetime, timedelta
import os
import warnings
warnings.filterwarnings('ignore')

from prophet import Prophet
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.api import SimpleExpSmoothing
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.tsa.holtwinters import Holt
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from pmdarima import auto_arima
import xgboost as xgb

import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT


# ì˜¤ëŠ˜ ë‚ ì§œ
TODAY = datetime.now().date()
CURRENT_YEAR = TODAY.year
CURRENT_MONTH = TODAY.month
CURRENT_DAY = TODAY.day

print(f"Today's date: {TODAY}")
print(f"Training improved models to predict from {TODAY} onwards")


# PostgreSQL ì—°ê²°
conn_ps = psycopg2.connect(
    host="15.164.112.237", 
    database="dify", 
    user="difyuser", 
    password="bico0218"
)

conn_ps.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
cursor_ps = conn_ps.cursor()

query = """
    SELECT ë§ˆìŠ¤í„°_SKU, ìˆ˜ëŸ‰, ì‹œì 
    FROM playauto_shipment_receipt
    WHERE ì…ì¶œê³ _ì—¬ë¶€ = 'ì¶œê³ '
    ORDER BY ë§ˆìŠ¤í„°_SKU, ì‹œì 
"""
cursor_ps.execute(query)
result = cursor_ps.fetchall()


# ë°ì´í„°
df = pd.DataFrame(result, columns=['ë§ˆìŠ¤í„°_SKU', 'ìˆ˜ëŸ‰', 'ì‹œì '])

# ë°ì´í„° ì²˜ë¦¬
df['ì‹œì '] = pd.to_datetime(df['ì‹œì '])
df['ë‚ ì§œ'] = df['ì‹œì '].dt.date

# IMPORTANT: í˜„ì¬ ì›”ì€ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì œì™¸
TRAINING_CUTOFF = datetime(CURRENT_YEAR, CURRENT_MONTH, 1).date() - timedelta(days=1)
print(f"Using data up to {TRAINING_CUTOFF} for training")

# í›ˆë ¨ ë°ì´í„°
df_train = df[df['ë‚ ì§œ'] <= TRAINING_CUTOFF].copy()

# í˜„ì¬ ì›” ì‹¤ì œ ë°ì´í„° ì €ì¥ (ë¹„êµìš©)
df_current_month = df[(df['ë‚ ì§œ'] >= datetime(CURRENT_YEAR, CURRENT_MONTH, 1).date()) & 
                      (df['ë‚ ì§œ'] <= TODAY)].copy()
df_current_month_by_sku = df_current_month.groupby('ë§ˆìŠ¤í„°_SKU')['ìˆ˜ëŸ‰'].sum().to_dict()

# í•™ìŠµì€ sku ë° ì›”ë³„ë¡œ ë¬¶ì–´ì„œ í•™ìŠµ
df_train['ì—°ì›”'] = df_train['ì‹œì '].dt.to_period('M')
df_monthly = df_train.groupby(['ë§ˆìŠ¤í„°_SKU', 'ì—°ì›”'])['ìˆ˜ëŸ‰'].sum().reset_index()
df_monthly['ë‚ ì§œ'] = df_monthly['ì—°ì›”'].dt.to_timestamp()

# ì¼ ë‹¨ìœ„ë¡œ ë¬¶ê¸°: ë‹¨ê¸°ê°„ íŠ¸ë Œë“œ ë° íœ˜ë°œì„± ëª©ì 
df_daily = df_train.groupby(['ë§ˆìŠ¤í„°_SKU', 'ë‚ ì§œ'])['ìˆ˜ëŸ‰'].sum().reset_index()

# ì£¼ ë‹¨ìœ„ë¡œ ë¬¶ê¸°: íŒ¨í„´ íŒŒì•… ëª©ì 
df_train['ì£¼'] = df_train['ì‹œì '].dt.to_period('W')
df_weekly = df_train.groupby(['ë§ˆìŠ¤í„°_SKU', 'ì£¼'])['ìˆ˜ëŸ‰'].sum().reset_index()
df_weekly['ë‚ ì§œ'] = df_weekly['ì£¼'].dt.to_timestamp()

# ë§ˆìŠ¤í„° SKU ë¦¬ìŠ¤íŠ¸
all_skus = ['BIOBAL', 'PSBAL', 'CLBAL', 'NEUROMASTER', 'KNCORE', 'DARAECARE', 'SF', 'YOUNGDAYS', 'DDCARE', 'KDDWAY', 'PMPKOR']

# Process each SKU and categorize
sku_data_monthly = {}
sku_data_daily = {}
sku_data_weekly = {}
sku_categories = {}

for sku in all_skus:  # ëª¨ë“  ì œí’ˆì— ëŒ€í•´ ë‘˜ëŸ¬ë³´ê¸°
    # ì›”ë³„ ë°ì´í„°
    sku_df_monthly = df_monthly[df_monthly['ë§ˆìŠ¤í„°_SKU'] == sku].copy()
    if len(sku_df_monthly) > 0:
        sku_df_monthly = sku_df_monthly.sort_values('ë‚ ì§œ')
        sku_data_monthly[sku] = sku_df_monthly
        
        # Daily data
        sku_df_daily = df_daily[df_daily['ë§ˆìŠ¤í„°_SKU'] == sku].copy()
        sku_df_daily = sku_df_daily.sort_values('ë‚ ì§œ')
        sku_data_daily[sku] = sku_df_daily
        
        # Weekly data
        sku_df_weekly = df_weekly[df_weekly['ë§ˆìŠ¤í„°_SKU'] == sku].copy()
        sku_df_weekly = sku_df_weekly.sort_values('ë‚ ì§œ')
        sku_data_weekly[sku] = sku_df_weekly
        
        # ì›”ë‹¨ìœ„ë¡œ í•™ìŠµ
        data_points = len(sku_df_monthly)
        avg_quantity = sku_df_monthly['ìˆ˜ëŸ‰'].mean()
        std_quantity = sku_df_monthly['ìˆ˜ëŸ‰'].std()
        cv = std_quantity / avg_quantity if avg_quantity > 0 else float('inf')  # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€ í¸ì°¨ / ì‚°ìˆ  í‰ê· 
        
        # íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•´ ì„ í˜•íšŒê·€ ì´ìš©
        if len(sku_df_monthly) >= 3:
            X = np.arange(len(sku_df_monthly)).reshape(-1, 1)
            y = sku_df_monthly['ìˆ˜ëŸ‰'].values
            lr = LinearRegression().fit(X, y)  # ì„ í˜•íšŒê·€ë¡œ íŠ¸ë Œë“œ ë¶„ì„
            trend_slope = lr.coef_[0]
            trend_strength = abs(trend_slope) / avg_quantity if avg_quantity > 0 else 0
        else:
            trend_slope = 0
            trend_strength = 0
        
        # ë°ì´í„° ê°œìˆ˜: í•™ìŠµ ë°ì´í„°ì— ëª‡ ì›”ì´ë‚˜ ìˆëŠ”ì§€
        # More lenient categorization for better predictions
        if data_points < 3:
            category = 'INSUFFICIENT_DATA'  # Less than minimum required
        elif data_points <= 5:
            category = 'LIMITED_DATA'
        elif data_points <= 12:
            category = 'MODERATE_DATA'
        else:
            category = 'SUFFICIENT_DATA'
        
        # ë³€ë™ì„± í™•ì¸
        if cv > 1.0:
            volatility = 'HIGH'
        elif cv > 0.5:
            volatility = 'MEDIUM'
        else:
            volatility = 'LOW'
        
        # íŠ¸ë Œë“œ ë¶„ì„: ìƒìŠ¹/í•˜ë½
        if trend_strength > 0.1:
            trend_type = 'UPWARD' if trend_slope > 0 else 'DOWNWARD'  # ìƒìŠ¹/í•˜ë½
        else:
            trend_type = 'STABLE'  # ê·¸ëŒ€ë¡œ ìœ ì§€
        
        sku_categories[sku] = {
            'category': category,  # ë°ì´í„° ê°œìˆ˜(ì›” ê°œìˆ˜)
            'volatility': volatility,  # ë³€ë™ì„±
            'trend_type': trend_type,  # ìƒìŠ¹/í•˜ë½/ìœ ì§€
            'trend_strength': trend_strength,
            'data_points': data_points,  # ë°ì´í„° ê°œìˆ˜(ì›” ê°œìˆ˜)
            'avg_monthly_quantity': avg_quantity,
            'std_monthly_quantity': std_quantity,
            'cv': cv,
            'total_days_data': len(sku_df_daily),
            'total_weeks_data': len(sku_df_weekly)
        }
        
        print(f"Processed {sku}: {data_points} months, {len(sku_df_weekly)} weeks, {len(sku_df_daily)} days")
        print(f"  Stats: Avg={avg_quantity:.0f}, CV={cv:.2f}, Trend={trend_type}")

print("\n" + "="*60)
print("IMPROVED ADAPTIVE MODEL TRAINING")
print("="*60)

class ImprovedAdaptiveModelSelector:
    def __init__(self, sku_name, monthly_data, daily_data, weekly_data, category_info):
        self.sku_name = sku_name
        self.monthly_data = monthly_data
        self.daily_data = daily_data
        self.weekly_data = weekly_data
        self.category = category_info['category']
        self.volatility = category_info['volatility']
        self.trend_type = category_info['trend_type']
        self.data_points = category_info['data_points']
        self.avg_monthly_quantity = category_info['avg_monthly_quantity']
        
    def select_and_train(self):
        """Select best model using cross-validation and predict future months"""
        
        days_in_month1 = 31  # Jan, Mar, May, Jul, Aug, Oct, Dec
        days_in_month2 = 30  # April, Jun, Sep, Nov
        days_in_february = 28
        remaining_month1_days = days_in_month1 - CURRENT_DAY + 1
        
        print(f"  Predicting full months: August, September, October, November")
        
        # Note: INSUFFICIENT_DATA is handled before calling this method
        if self.category == 'INSUFFICIENT_DATA':  # Less than 6 months
            return self._minimal_data_strategy(remaining_month1_days)
        elif self.category == 'LIMITED_DATA':  # 6~8ê°œì›”
            return self._limited_data_strategy(remaining_month1_days)
        elif self.category == 'MODERATE_DATA':  # 9~12ê°œì›”
            return self._moderate_data_strategy(remaining_month1_days)
        else:  # SUFFICIENT_DATA  # 12ê°œì›” ì´ˆê³¼
            return self._sufficient_data_strategy(remaining_month1_days)
    
    def _detect_seasonality(self, data, freq=12):
        """Detect if there's seasonality in the data"""
        if len(data) < freq * 2:
            return False, None
        
        try:
            # Use autocorrelation to detect seasonality
            acf_values = acf(data, nlags=min(freq+1, len(data)//2))  # acf???
            # Check if there's significant correlation at seasonal lag
            if len(acf_values) > freq and abs(acf_values[freq]) > 0.3:
                return True, freq
        except:
            pass
        
        return False, None
    
    def _minimal_data_strategy(self, remaining_month1_days):
        """Enhanced strategy for minimal data using growth patterns"""
        y = self.monthly_data['ìˆ˜ëŸ‰'].values
        
        # Look at daily patterns if available
        if len(self.daily_data) > 0:
            # Calculate trend from daily data
            daily_vals = self.daily_data['ìˆ˜ëŸ‰'].values
            if len(daily_vals) > 7:
                # 7ì¼ í˜¹ì€ 14ì¼ê°„ ë°ì´í„° ì´ìš©
                recent_avg = np.mean(daily_vals[-7:])  # ì§€ë‚œ 7ì¼ (ìµœê·¼)
                prev_avg = np.mean(daily_vals[-14:-7]) if len(daily_vals) >= 14 else np.mean(daily_vals[:-7])  # 2ì£¼ ì „
                growth_rate = (recent_avg - prev_avg) / prev_avg if prev_avg > 0 else 0
            else:
                growth_rate = 0
            
            daily_avg = np.mean(daily_vals) * (1 + growth_rate * 0.5)  # ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë³€ë™ ë°©ì§€ ëª©ì ìœ¼ë¡œ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
        else:
            daily_avg = np.mean(y) / 30
        
        # Apply minimum threshold
        daily_avg = max(0.3, daily_avg)
        
        # Add slight randomness(noise) for realism
        np.random.seed(42)
        noise_factor = 1 + np.random.normal(0, 0.05, 4)  # í˜„ì‹¤ì„±ì„ ìœ„í•´ ì˜ˆì¸¡ ê²°ê³¼ê°€ ë„ˆë¬´ í‰í‰í•˜ê²Œ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ë…¸ì´ì¦ˆ ì¶”ê°€
        
        # í•´ë‹¹ ì›”ì— ë”°ë¼ [28, 29, 30, 31]ì¼ ë§ì¶”ë„ë¡ í•´ë³´ì
        thismonth_full = daily_avg * 30 * noise_factor[0]  # Full August prediction
        month1_total = daily_avg * 30 * noise_factor[1]
        month2_total = daily_avg * 30 * noise_factor[2]
        month3_total = daily_avg * 30 * noise_factor[3]
        
        return {
            'method': 'minimal_data_enhanced',
            'predictions': {
                'august_full': max(0, thismonth_full),
                'august_remainder': max(0, thismonth_full * remaining_month1_days / 31),
                'september': max(0, month1_total),
                'october': max(0, month2_total),
                'november': max(0, month3_total)
            },
            'daily_rate': daily_avg,
            'confidence': 'low'
        }
    
    def _limited_data_strategy(self, remaining_month1_days):
        """Enhanced strategy using exponential smoothing and trend analysis"""
        y = self.monthly_data['ìˆ˜ëŸ‰'].values
        
        models_results = []
        
        # 1. Exponential smoothing with optimized alpha
        best_alpha = None
        best_mse = float('inf')
        
        for alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
            try:  # ê°€ì¥ ìµœê·¼ ë°ì´í„°ì¼ìˆ˜ë¡£ ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš©
                ses = SimpleExpSmoothing(y)
                ses_fit = ses.fit(smoothing_level=alpha)
                
                # Simple validation on last point
                if len(y) > 2:
                    train_y = y[:-1]
                    test_y = y[-1]
                    ses_train = SimpleExpSmoothing(train_y).fit(smoothing_level=alpha)
                    pred = ses_train.forecast(1)[0]
                    mse = (pred - test_y) ** 2
                    
                    if mse < best_mse:
                        best_mse = mse
                        best_alpha = alpha
            except:
                continue
        
        if best_alpha:
            ses = SimpleExpSmoothing(y).fit(smoothing_level=best_alpha)
            base_forecast = ses.forecast(4)
            base_forecast = np.maximum(base_forecast, 0)
            
            # Adjust for trend
            if len(y) >= 3:
                recent_trend = (y[-1] - y[-3]) / 2
                trend_factor = recent_trend / np.mean(y) if np.mean(y) > 0 else 0
                base_forecast = base_forecast * (1 + trend_factor * np.array([0.3, 0.2, 0.1, 0.05]))
            
            models_results.append(base_forecast)
        
        # 2. ì‹œê³„ì—´ ê¸°ë°˜ ì„ í˜• íšŒê·€ í•™ìŠµ
        if len(y) >= 3:
            X = np.arange(len(y)).reshape(-1, 1)
            lr = LinearRegression().fit(X, y)
            future_X = np.arange(len(y), len(y) + 4).reshape(-1, 1)
            lr_forecast = lr.predict(future_X)
            lr_forecast = np.maximum(lr_forecast, 0)
            models_results.append(lr_forecast)
        
        # ì•™ìƒë¸”: Exponential smoothing + linear regression
        if models_results:
            ensemble_forecast = np.mean(models_results, axis=0)
        else:
            daily_avg = np.mean(y) / 30
            ensemble_forecast = np.array([daily_avg * 31, daily_avg * 30, daily_avg * 31, daily_avg * 30])
        
        # Convert to daily and calculate
        thismonth_full = ensemble_forecast[0]  # Full this month prediction
        thismonth_daily = thismonth_full / 31
        
        return {
            'method': 'limited_data_ensemble',
            'predictions': {
                'august_full': thismonth_full,
                'august_remainder': thismonth_daily * remaining_month1_days,
                'september': ensemble_forecast[1],
                'october': ensemble_forecast[2],
                'november': ensemble_forecast[3]
            },
            'confidence': 'low-medium'
        }
    
    def _moderate_data_strategy(self, remaining_month1_days):
        """Strategy using Auto-ARIMA and seasonal decomposition"""
        y = self.monthly_data['ìˆ˜ëŸ‰'].values
        
        models_results = []
        
        # 1. Auto-ARIMA í•™ìŠµ
        try:
            auto_model = auto_arima(
                y, 
                start_p=1, start_q=1,  # Start from 1 to avoid (0,0,0)
                max_p=5, max_q=5,      # Increase search space
                seasonal=False,  # Not enough data for seasonal
                stepwise=True,
                suppress_warnings=True,
                error_action='ignore',
                n_jobs=-1,
                d=None,  # Let it determine differencing
                max_d=2,
                trace=False,
                scoring='mse',
                with_intercept=True
            )
            
            auto_forecast = auto_model.predict(n_periods=4)
            auto_forecast = np.maximum(auto_forecast, 0)
            models_results.append(auto_forecast)
            print(f"    Auto-ARIMA order: {auto_model.order}")
        except Exception as e:
            print(f"    Auto-ARIMA failed: {str(e)[:50]}")
        
        # 2. í™€íŠ¸ ëª¨ë¸ í•™ìŠµ (Holt's Linear Trend)
        try:
            holt_model = Holt(y, exponential=False, damped=True)
            holt_fit = holt_model.fit(optimized=True)
            holt_forecast = holt_fit.forecast(4)
            holt_forecast = np.maximum(holt_forecast, 0)
            models_results.append(holt_forecast)
        except:
            pass
        
        # 3. Prophet ëª¨ë¸ í•™ìŠµ (Prophet with custom seasonality)
        try:
            prophet_df = pd.DataFrame({
                'ds': self.monthly_data['ë‚ ì§œ'],
                'y': y
            })
            
            # Add custom regressors if we have enough data
            if len(prophet_df) >= 6:
                prophet_df['time_trend'] = np.arange(len(prophet_df))
                prophet_df['time_trend_squared'] = prophet_df['time_trend'] ** 2
            
            prophet_model = Prophet(
                changepoint_prior_scale=0.05,
                seasonality_mode='additive',
                yearly_seasonality=False,
                weekly_seasonality=False,
                daily_seasonality=False,
                interval_width=0.95
            )
            
            if len(prophet_df) >= 6:
                prophet_model.add_regressor('time_trend')
                prophet_model.add_regressor('time_trend_squared')
            
            prophet_model.fit(prophet_df)
            
            future_dates = pd.date_range(
                start=prophet_df['ds'].max() + pd.DateOffset(months=1),
                periods=4,
                freq='MS'
            )
            future = pd.DataFrame({'ds': future_dates})
            
            if len(prophet_df) >= 6:
                future['time_trend'] = np.arange(len(prophet_df), len(prophet_df) + 4)
                future['time_trend_squared'] = future['time_trend'] ** 2
            
            forecast = prophet_model.predict(future)
            prophet_forecast = forecast['yhat'].values
            prophet_forecast = np.maximum(prophet_forecast, 0)
            models_results.append(prophet_forecast)
        except:
            pass
        
        # Weighted ensemble
        if len(models_results) >= 2:
            # Give more weight to recent performance
            weights = np.array([0.4, 0.35, 0.25])[:len(models_results)]
            weights = weights / weights.sum()
            ensemble_forecast = np.average(models_results, axis=0, weights=weights)
            
            # Apply more flexible bounds based on historical data
            historical_max = np.percentile(y, 95) * 1.5  # More lenient upper bound
            historical_min = max(0, np.percentile(y, 5) * 0.5)  # More lenient lower bound
            ensemble_forecast = np.clip(ensemble_forecast, historical_min, historical_max)
            
            # Add trend component if detected
            if self.trend_type != 'STABLE':
                trend_multiplier = 1.0
                for i in range(len(ensemble_forecast)):
                    if self.trend_type == 'UPWARD':
                        trend_multiplier *= 1.02  # 2% monthly growth
                    else:  # DOWNWARD
                        trend_multiplier *= 0.98  # 2% monthly decline
                    ensemble_forecast[i] *= trend_multiplier
        elif models_results:
            ensemble_forecast = models_results[0]
        else:
            # Fallback
            daily_avg = np.mean(y) / 30
            ensemble_forecast = np.array([daily_avg * 31, daily_avg * 30, daily_avg * 31, daily_avg * 30])
        
        august_full = ensemble_forecast[0]  # Full August prediction
        august_daily = august_full / 31
        
        return {
            'method': 'moderate_data_autoarima_ensemble',
            'predictions': {
                'august_full': august_full,
                'august_remainder': august_daily * remaining_month1_days,
                'september': ensemble_forecast[1],
                'october': ensemble_forecast[2],
                'november': ensemble_forecast[3]
            },
            'models_count': len(models_results),
            'confidence': 'medium'
        }
    
    def _sufficient_data_strategy(self, remaining_month1_days):
        """Advanced strategy with multiple models and cross-validation"""
        y = self.monthly_data['ìˆ˜ëŸ‰'].values
        
        # Check for seasonality
        has_seasonality, seasonal_period = self._detect_seasonality(y)
        
        models_results = []
        model_scores = []
        
        # Time series cross-validation setup
        n_splits = min(3, len(y) // 4)
        if n_splits >= 2:
            tscv = TimeSeriesSplit(n_splits=n_splits)
        else:
            tscv = None
        
        # 1. Auto-ARIMAë¡œ SARIMA ëª¨ë¸ êµ¬í˜„
        try:
            if has_seasonality and seasonal_period:
                auto_model = auto_arima(
                    y,
                    start_p=1, start_q=1,  # Start from 1
                    max_p=5, max_q=5,      # Wider search
                    start_P=0, start_Q=0,
                    max_P=2, max_Q=2,
                    m=seasonal_period,
                    seasonal=True,
                    stepwise=True,
                    suppress_warnings=True,
                    error_action='ignore',
                    n_jobs=-1,
                    d=None,
                    max_d=2,
                    with_intercept=True
                )
            else:
                auto_model = auto_arima(
                    y,
                    start_p=1, start_q=1,  # Start from 1
                    max_p=5, max_q=5,      # Wider search
                    seasonal=False,
                    stepwise=True,
                    suppress_warnings=True,
                    error_action='ignore',
                    n_jobs=-1,
                    d=None,
                    max_d=2,
                    with_intercept=True
                )
            
            sarima_forecast = auto_model.predict(n_periods=4)
            sarima_forecast = np.maximum(sarima_forecast, 0)
            
            # Cross-validation score
            if tscv:
                cv_scores = []
                for train_idx, test_idx in tscv.split(y):
                    try:
                        train_y = y[train_idx]
                        test_y = y[test_idx]
                        temp_model = auto_model.__class__(order=auto_model.order)
                        temp_model.fit(train_y)
                        pred = temp_model.predict(n_periods=len(test_y))
                        cv_scores.append(mean_absolute_percentage_error(test_y, pred))
                    except:
                        continue
                
                if cv_scores:
                    model_scores.append(np.mean(cv_scores))
                else:
                    model_scores.append(1.0)
            else:
                model_scores.append(1.0)
            
            models_results.append(sarima_forecast)
            print(f"    SARIMA order: {auto_model.order}, seasonal: {has_seasonality}")
        except Exception as e:
            print(f"    SARIMA failed: {str(e)[:50]}")
        
        # 2. Holt-Winters with optimization
        if len(y) >= 12:
            try:
                hw = ExponentialSmoothing(
                    y, 
                    trend='add', 
                    seasonal='add' if has_seasonality else None,
                    seasonal_periods=seasonal_period if has_seasonality else None
                )
                hw_fit = hw.fit(optimized=True)
                hw_forecast = hw_fit.forecast(4)
                hw_forecast = np.maximum(hw_forecast, 0)
                
                # Validation
                if tscv:
                    cv_scores = []
                    for train_idx, test_idx in tscv.split(y):
                        try:
                            train_y = y[train_idx]
                            test_y = y[test_idx]
                            temp_hw = ExponentialSmoothing(
                                train_y,
                                trend='add',
                                seasonal='add' if has_seasonality and len(train_y) >= seasonal_period else None,
                                seasonal_periods=seasonal_period if has_seasonality else None
                            )
                            temp_fit = temp_hw.fit()
                            pred = temp_fit.forecast(len(test_y))
                            cv_scores.append(mean_absolute_percentage_error(test_y, pred))
                        except:
                            continue
                    
                    if cv_scores:
                        model_scores.append(np.mean(cv_scores))
                    else:
                        model_scores.append(1.0)
                else:
                    model_scores.append(1.0)
                
                models_results.append(hw_forecast)
            except:
                pass
        
        # 3. XGBoost ëª¨ë¸ í•™ìŠµ(with time features)
        try:
            # Prepare features
            X_train = []
            for i in range(3, len(y)):
                features = [
                    y[i-1],  # lag 1
                    y[i-2],  # lag 2
                    y[i-3],  # lag 3
                    np.mean(y[max(0, i-6):i]),  # 6-month MA
                    np.std(y[max(0, i-6):i]),  # 6-month std
                    i,  # time index
                    i % 12,  # month of year
                ]
                X_train.append(features)
            
            X_train = np.array(X_train)
            y_train = y[3:]
            
            if len(X_train) > 0:
                xgb_model = xgb.XGBRegressor(
                    n_estimators=100,
                    max_depth=3,
                    learning_rate=0.1,
                    random_state=42
                )
                xgb_model.fit(X_train, y_train)
                
                # Predict future
                xgb_forecast = []
                last_values = list(y[-3:])
                
                for step in range(4):
                    features = [
                        last_values[-1],
                        last_values[-2],
                        last_values[-3],
                        np.mean(last_values[-6:]) if len(last_values) >= 6 else np.mean(last_values),
                        np.std(last_values[-6:]) if len(last_values) >= 6 else np.std(last_values),
                        len(y) + step,
                        (len(y) + step) % 12,
                    ]
                    pred = xgb_model.predict(np.array([features]))[0]
                    pred = max(0, pred)
                    xgb_forecast.append(pred)
                    last_values.append(pred)
                
                xgb_forecast = np.array(xgb_forecast)
                models_results.append(xgb_forecast)
                model_scores.append(0.8)  # Default score for XGBoost
        except:
            pass
        
        # 4. Prophet ëª¨ë¸ í•™ìŠµ(with enhanced features)
        try:
            prophet_df = pd.DataFrame({
                'ds': self.monthly_data['ë‚ ì§œ'],
                'y': y
            })
            
            # Add trend and polynomial features
            prophet_df['time_trend'] = np.arange(len(prophet_df))
            prophet_df['time_trend_squared'] = prophet_df['time_trend'] ** 2
            
            # Add lagged features if enough data
            if len(prophet_df) >= 6:
                prophet_df['lag1'] = prophet_df['y'].shift(1).fillna(prophet_df['y'].mean())
                prophet_df['lag3'] = prophet_df['y'].shift(3).fillna(prophet_df['y'].mean())
            
            prophet_model = Prophet(
                changepoint_prior_scale=0.1,
                seasonality_mode='multiplicative' if self.volatility == 'HIGH' else 'additive',
                yearly_seasonality=len(y) >= 24,
                weekly_seasonality=False,
                daily_seasonality=False,
                interval_width=0.95
            )
            
            prophet_model.add_regressor('time_trend')
            prophet_model.add_regressor('time_trend_squared')
            
            if len(prophet_df) >= 6:
                prophet_model.add_regressor('lag1')
                prophet_model.add_regressor('lag3')
            
            # Add monthly seasonality if detected
            if has_seasonality:
                prophet_model.add_seasonality(
                    name='monthly',
                    period=30.5,
                    fourier_order=3
                )
            
            prophet_model.fit(prophet_df)
            
            # Prepare future dataframe
            future_dates = pd.date_range(
                start=prophet_df['ds'].max() + pd.DateOffset(months=1),
                periods=4,
                freq='MS'
            )
            future = pd.DataFrame({'ds': future_dates})
            future['time_trend'] = np.arange(len(prophet_df), len(prophet_df) + 4)
            future['time_trend_squared'] = future['time_trend'] ** 2
            
            if len(prophet_df) >= 6:
                # Use last known values for lags
                future['lag1'] = y[-1]
                future['lag3'] = y[-3] if len(y) >= 3 else y[-1]
            
            forecast = prophet_model.predict(future)
            prophet_forecast = forecast['yhat'].values
            prophet_forecast = np.maximum(prophet_forecast, 0)
            
            models_results.append(prophet_forecast)
            model_scores.append(0.9)  # Default score for Prophet
        except Exception as e:
            print(f"    Prophet failed: {str(e)[:50]}")
        
        # Weighted ensemble based on cross-validation scores
        if len(models_results) >= 2:
            # Convert scores to weights (lower is better)
            if model_scores:
                weights = 1 / np.array(model_scores[:len(models_results)])
                weights = weights / weights.sum()
            else:
                weights = np.ones(len(models_results)) / len(models_results)
            
            ensemble_forecast = np.average(models_results, axis=0, weights=weights)
            
            # Apply more flexible adaptive bounds
            recent_mean = np.mean(y[-6:]) if len(y) >= 6 else np.mean(y)
            recent_std = np.std(y[-6:]) if len(y) >= 6 else np.std(y)
            
            upper_bound = recent_mean + 3 * recent_std  # More lenient
            lower_bound = max(0, recent_mean - 3 * recent_std)
            
            ensemble_forecast = np.clip(ensemble_forecast, lower_bound, upper_bound)
            
            # Allow more natural transitions between months
            # Remove the overly restrictive smoothing that was making predictions flat
            
            method = f'advanced_weighted_ensemble_{len(models_results)}_models'
            confidence = 'high' if len(models_results) >= 3 else 'medium-high'
        elif models_results:
            ensemble_forecast = models_results[0]
            method = 'single_advanced_model'
            confidence = 'medium'
        else:
            # Fallback to moderate strategy
            return self._moderate_data_strategy(remaining_month1_days)
        
        # Convert monthly to daily predictions
        august_full = ensemble_forecast[0]  # Full August prediction
        august_daily = august_full / 31
        
        return {
            'method': method,
            'predictions': {
                'august_full': august_full,
                'august_remainder': august_daily * remaining_month1_days,
                'september': ensemble_forecast[1],
                'october': ensemble_forecast[2],
                'november': ensemble_forecast[3]
            },
            'models_count': len(models_results),
            'confidence': confidence,
            'model_weights': weights.tolist() if len(models_results) >= 2 else None
        }
    
    def backtest_model(self):  # ì„±ëŠ¥ í‰ê°€
        """Perform backtesting to calculate RMSE and MAE"""
        y = self.monthly_data['ìˆ˜ëŸ‰'].values
        
        if len(y) < 4:  # Need at least 4 months for meaningful backtesting
            return {'RMSE': None, 'MAE': None, 'test_periods': 0}
        
        errors = []
        test_sizes = min(3, len(y) // 2)  # Test on last 1-3 months
        
        for test_size in range(1, test_sizes + 1):
            if len(y) - test_size < 3:  # Need at least 3 months for training
                continue
                
            train_y = y[:-test_size]
            test_y = y[-test_size:]
            
            # Use same strategy as main prediction
            if len(train_y) <= 2:
                # Simple average for minimal data
                pred = np.full(test_size, np.mean(train_y))
            elif len(train_y) <= 5:
                # Weighted average with trend for limited data
                weights = [0.5, 0.3, 0.2][-len(train_y):]
                pred_val = np.average(train_y[-len(weights):], weights=weights)
                # Add slight variation
                pred = np.array([pred_val * (1 + 0.02 * i) for i in range(test_size)])
            else:
                # Try Auto-ARIMA for sufficient data
                try:
                    auto_model = auto_arima(
                        train_y,
                        start_p=1, start_q=1,  # Start from 1
                        max_p=3, max_q=3,
                        seasonal=False,
                        stepwise=True,
                        suppress_warnings=True,
                        error_action='ignore',
                        d=None,
                        max_d=2
                    )
                    pred = auto_model.predict(n_periods=test_size)
                except:
                    # Fallback to exponential smoothing
                    try:
                        ses = SimpleExpSmoothing(train_y).fit(smoothing_level=0.3)
                        pred = ses.forecast(test_size)
                    except:
                        pred = np.full(test_size, np.mean(train_y))
            
            pred = np.maximum(pred, 0)  # Ensure non-negative
            
            # ì˜¤ì°¨ ê³„ì‚°
            errors.append({
                'predictions': pred,
                'actuals': test_y,
                'rmse': np.sqrt(mean_squared_error(test_y, pred)),
                'mae': mean_absolute_error(test_y, pred)
            })
        
        if errors:
            avg_rmse = np.mean([e['rmse'] for e in errors])
            avg_mae = np.mean([e['mae'] for e in errors])
            return {
                'RMSE': avg_rmse,
                'MAE': avg_mae,
                'test_periods': len(errors),
                'details': errors
            }
        else:
            return {'RMSE': None, 'MAE': None, 'test_periods': 0}

# Train models with improved strategies
model_results = {}
trained_models = {}
future_predictions = {}

for sku in all_skus:
    if sku not in sku_data_monthly:
        continue
        
    print(f"\n{'='*50}")
    print(f"SKU: {sku}")
    print(f"Data points: {sku_categories[sku]['data_points']} months")
    
    # Check minimum data requirement - more reasonable threshold
    MIN_MONTHS_FOR_TRAINING = 3  # Minimum for any model training
    MIN_MONTHS_FOR_PREDICTION = 2  # Absolute minimum to show any prediction
    
    if sku_categories[sku]['data_points'] < MIN_MONTHS_FOR_PREDICTION:
        print(f"âŒ SKIPPING: Only {sku_categories[sku]['data_points']} months available")
        print(f"  Minimum {MIN_MONTHS_FOR_PREDICTION} months required for any prediction")
        print(f"  This product will not have predictions available")
        
        # Skip this SKU entirely - no predictions
        continue
        
    elif sku_categories[sku]['data_points'] < MIN_MONTHS_FOR_TRAINING:
        print(f"âš ï¸ LIMITED DATA: Only {sku_categories[sku]['data_points']} months available")
        print(f"  Using simple baseline approach (minimum {MIN_MONTHS_FOR_TRAINING} months for model training)")
        
        # Simple baseline for 5 months of data only
        monthly_data = sku_data_monthly[sku]['ìˆ˜ëŸ‰'].values
        
        # Use weighted average favoring recent data
        weights = np.array([0.5, 0.3, 0.2])[-len(monthly_data):]
        baseline_value = np.average(monthly_data[-len(weights):], weights=weights)
        
        # Very conservative growth for baseline
        result = {
            'method': 'baseline_limited_data',
            'predictions': {
                'august_full': baseline_value,
                'august_remainder': baseline_value * 19 / 31,
                'september': baseline_value * 1.02,
                'october': baseline_value * 1.03,
                'november': baseline_value * 1.04
            },
            'confidence': 'low',
            'warning': f'Only {sku_categories[sku]["data_points"]} months of data - using baseline'
        }
        
        # No backtesting for baseline
        backtest_results = {'RMSE': None, 'MAE': None, 'test_periods': 0}
        
    else:
        # Sufficient data - proceed with normal training
        print(f"Training improved models for SKU: {sku}")
        print(f"Category: {sku_categories[sku]['category']}")
        print(f"Volatility: {sku_categories[sku]['volatility']}")
        print(f"Trend: {sku_categories[sku]['trend_type']}")
        
        # Train model
        selector = ImprovedAdaptiveModelSelector(
            sku, 
            sku_data_monthly[sku], 
            sku_data_daily.get(sku, pd.DataFrame()),
            sku_data_weekly.get(sku, pd.DataFrame()),
            sku_categories[sku]
        )
        result = selector.select_and_train()
        
        # ëª¨ë¸ ì„±ëŠ¥ ì–»ê¸° (RMSE, MAE)
        backtest_results = selector.backtest_model()
    
    # Store results with performance metrics
    trained_models[sku] = {
        'method': result['method'],
        'confidence': result['confidence'],
        'category_info': sku_categories[sku],
        'models_count': result.get('models_count', 1),
        'RMSE': backtest_results['RMSE'],
        'MAE': backtest_results['MAE'],
        'test_periods': backtest_results['test_periods']
    }
    
    # Store model_results for app.py
    model_results[sku] = {
        'best_model': result['method'],
        'rmse': backtest_results['RMSE'] if backtest_results['RMSE'] else 0,
        'mae': backtest_results['MAE'] if backtest_results['MAE'] else 0,
        'confidence': result['confidence'],
        'test_periods': backtest_results['test_periods'],
        'models_used': result.get('models_count', 1)
    }
    
    # Convert predictions to array format
    predictions_array = np.array([
        result['predictions']['august_full'],  # Full August for comparison
        result['predictions']['september'],
        result['predictions']['october'],
        result['predictions']['november']
    ])
    
    # Calculate future trend based on predictions
    # Compare next 3 full months (Sep, Oct, Nov) to see trend
    future_months = predictions_array[1:]  # Sep, Oct, Nov
    
    # Linear regression on future predictions to determine trend
    X_future = np.arange(len(future_months)).reshape(-1, 1)
    y_future = future_months
    lr_future = LinearRegression().fit(X_future, y_future)
    future_trend_slope = lr_future.coef_[0]
    
    # Calculate trend strength and direction
    avg_future_quantity = np.mean(future_months)
    future_trend_strength = abs(future_trend_slope) / avg_future_quantity if avg_future_quantity > 0 else 0
    
    # Determine future trend type
    if future_trend_strength > 0.1:
        future_trend_type = 'INCREASING' if future_trend_slope > 0 else 'DECREASING'
    else:
        future_trend_type = 'STABLE'
    
    # Calculate month-to-month change rate
    month_changes = []
    if result['predictions']['september'] > 0:
        sep_change = ((result['predictions']['september'] - sku_categories[sku]['avg_monthly_quantity']) / 
                      sku_categories[sku]['avg_monthly_quantity'] * 100)
        month_changes.append(sep_change)
    
    if result['predictions']['october'] > 0 and result['predictions']['september'] > 0:
        oct_change = ((result['predictions']['october'] - result['predictions']['september']) / 
                      result['predictions']['september'] * 100)
        month_changes.append(oct_change)
    
    if result['predictions']['november'] > 0 and result['predictions']['october'] > 0:
        nov_change = ((result['predictions']['november'] - result['predictions']['october']) / 
                      result['predictions']['october'] * 100)
        month_changes.append(nov_change)
    
    avg_change_rate = np.mean(month_changes) if month_changes else 0
    
    # Check if ordering alert needed (significant increase expected)
    order_alert_needed = (future_trend_type == 'INCREASING' and future_trend_strength > 0.15) or \
                        (avg_change_rate > 20)  # More than 20% average increase
    
    future_predictions[sku] = {
        'predictions': predictions_array,
        'august_full_prediction': result['predictions']['august_full'],  # Store full current month separately  # current_full_prediction
        'august_remainder_prediction': result['predictions']['august_remainder'],  # Remaining days  # current_remainder_prediction
        'method': result['method'],
        'confidence': result['confidence'],
        'last_date': sku_data_monthly[sku]['ë‚ ì§œ'].max(),
        'forecast_months': ['august', 'september', 'october', 'november'],  # Changed to full months
        'august_days_included': 31 - CURRENT_DAY + 1,
        'prediction_start_date': TODAY,
        'models_count': result.get('models_count', 1),
        'model_weights': result.get('model_weights', None),
        'future_trend': {
            'type': future_trend_type,
            'strength': future_trend_strength,
            'slope': future_trend_slope,
            'avg_change_rate': avg_change_rate,
            'month_changes': month_changes,
            'order_alert': order_alert_needed
        }
    }
    
    # Show results based on method type
    if result['method'] == 'baseline_insufficient_data':
        print(f"Method: BASELINE (Insufficient Data)")
        print(f"âš ï¸ WARNING: {result.get('warning', 'Limited data available')}")
        print(f"Confidence: {result['confidence']}")
        print(f"No model training performed - using simple average")
    else:
        print(f"Method: {result['method']}")
        print(f"Confidence: {result['confidence']}")
        print(f"Models used: {result.get('models_count', 1)}")
        
        # Show performance metrics
        if backtest_results['RMSE'] is not None:
            print(f"Backtest Performance:")
            print(f"  RMSE: {backtest_results['RMSE']:.2f}")
            print(f"  MAE: {backtest_results['MAE']:.2f}")
            print(f"  Test periods: {backtest_results['test_periods']}")
        else:
            print(f"Backtest: Insufficient data")
    
    print(f"Predictions (Full Months):")  # ì „ì²´ ì›” ì˜ˆì¸¡
    print(f"  August: {result['predictions']['august_full']:.1f}")  # current month
    print(f"  September: {result['predictions']['september']:.1f}")  # month1
    print(f"  October: {result['predictions']['october']:.1f}")  # month2
    print(f"  November: {result['predictions']['november']:.1f}")  # month3
    print(f"  Total (4 months): {predictions_array.sum():.1f}")
    
    # Show actual vs predicted for August
    actual_august = df_current_month_by_sku.get(sku, 0)  # actual_current
    predicted_august_so_far = result['predictions']['august_full'] * CURRENT_DAY / 31  # Pro-rated prediction  # predicted_current_so_far
    print(f"\nAugust Comparison (as of day {CURRENT_DAY}):")
    print(f"  Actual so far: {actual_august:.1f}")
    print(f"  Expected by now: {predicted_august_so_far:.1f}")
    performance = ((actual_august - predicted_august_so_far) / predicted_august_so_far * 100) if predicted_august_so_far > 0 else 0
    if performance > 10:
        print(f"  Performance: {performance:.1f}% ABOVE prediction â†‘")
    elif performance < -10:
        print(f"  Performance: {performance:.1f}% BELOW prediction â†“")
    else:
        print(f"  Performance: {performance:.1f}% (on track)")
    
    # Show future trend analysis
    print(f"Future Trend Analysis:")
    print(f"  Trend: {future_predictions[sku]['future_trend']['type']}")
    print(f"  Avg Change Rate: {future_predictions[sku]['future_trend']['avg_change_rate']:.1f}%")
    if future_predictions[sku]['future_trend']['order_alert']:
        print(f"  âš ï¸ ORDER ALERT: Significant increase expected - consider ordering more stock!")

# í•™ìŠµ ê²°ê³¼, ì˜ˆì¸¡ ì €ì¥
os.makedirs('models', exist_ok=True)

with open('models/trained_models.pkl', 'wb') as f:
    pickle.dump(trained_models, f)

with open('models/future_predictions.pkl', 'wb') as f:
    pickle.dump(future_predictions, f)

with open('models/sku_categories.pkl', 'wb') as f:
    pickle.dump(sku_categories, f)

# Save model_results for app.py
with open('models/model_results.pkl', 'wb') as f:
    pickle.dump(model_results, f)

# Create detailed summary report
summary_data = []
for sku in all_skus:
    if sku in future_predictions:
        pred_info = future_predictions[sku]
        cat_info = sku_categories[sku]
        
        row = {
            'SKU': sku,
            'Category': cat_info['category'],
            'Volatility': cat_info['volatility'],
            'Historical_Trend': cat_info['trend_type'],
            'Future_Trend': pred_info['future_trend']['type'],
            'Trend_Change_%': pred_info['future_trend']['avg_change_rate'],
            'Order_Alert': pred_info['future_trend']['order_alert'],
            'Historical_Months': cat_info['data_points'],
            'Avg_Monthly': cat_info['avg_monthly_quantity'],
            'CV': cat_info['cv'],
            'Method': pred_info['method'],
            'Models_Used': pred_info.get('models_count', 1),
            'Confidence': pred_info['confidence'],
            'RMSE': model_results[sku]['rmse'],
            'MAE': model_results[sku]['mae'],
            'Test_Periods': model_results[sku]['test_periods'],
            'Aug_Pred': pred_info['august_full_prediction'],  # current_full_pred
            'Sep_Pred': pred_info['predictions'][1],  # month1_pred
            'Oct_Pred': pred_info['predictions'][2],  # month2_pred
            'Nov_Pred': pred_info['predictions'][3],  # month3_pred
            'Total_Next_4M': pred_info['predictions'].sum()
        }
        summary_data.append(row)

summary_df = pd.DataFrame(summary_data)
summary_df.to_csv('models_adaptive_improved/prediction_summary.csv', index=False)

# Create comparison with previous month
comparison_data = []
for sku in all_skus:
    if sku in future_predictions and sku in sku_data_monthly:
        last_month_actual = sku_data_monthly[sku]['ìˆ˜ëŸ‰'].iloc[-1] if len(sku_data_monthly[sku]) > 0 else 0
        pred_avg = np.mean(future_predictions[sku]['predictions'][1:])  # Avg of Sep, Oct, Nov
        
        change_pct = ((pred_avg - last_month_actual) / last_month_actual * 100) if last_month_actual > 0 else 0
        
        comparison_data.append({
            'SKU': sku,
            'Last_Month_Actual': last_month_actual,
            'Predicted_Avg_Next_3M': pred_avg,
            'Change_Percent': change_pct
        })

comparison_df = pd.DataFrame(comparison_data)
comparison_df.to_csv('models_adaptive_improved/comparison_report.csv', index=False)

print("\n" + "="*60)
print("IMPROVED MODEL TRAINING COMPLETED")
print("="*60)

# Create summary of trained vs baseline vs skipped models
trained_count = 0
baseline_count = 0
skipped_count = 0
trained_skus = []
baseline_skus = []
skipped_skus = []

for sku in all_skus:
    if sku not in sku_data_monthly or sku_categories[sku]['data_points'] < 2:
        # Skipped due to insufficient data
        skipped_count += 1
        skipped_skus.append(f"{sku} ({sku_categories.get(sku, {}).get('data_points', 0)} months)")
    elif sku in trained_models:
        if trained_models[sku]['method'] in ['baseline_insufficient_data', 'baseline_limited_data']:
            baseline_count += 1
            baseline_skus.append(f"{sku} ({sku_categories[sku]['data_points']} months)")
        else:
            trained_count += 1
            trained_skus.append(sku)

print(f"\nğŸ“Š TRAINING SUMMARY:")
print(f"  - Models trained: {trained_count} SKUs")
print(f"  - Baseline used: {baseline_count} SKUs (<3 months data)")
print(f"  - Skipped: {skipped_count} SKUs (<2 months data)")

if skipped_skus:
    print(f"\nâŒ SKUs SKIPPED (<2 months data - no predictions):")
    for sku_info in skipped_skus:
        print(f"    - {sku_info}")

if baseline_skus:
    print(f"\nâš ï¸ SKUs using baseline (<3 months data):")
    for sku_info in baseline_skus:
        print(f"    - {sku_info}")

if trained_skus:
    print(f"\nâœ… SKUs with trained models:")
    for sku in trained_skus:
        print(f"    - {sku} ({sku_categories[sku]['data_points']} months)")

print(f"\nModels saved to 'models_adaptive_improved/' directory")
print(f"\nKey improvements implemented:")
print("  1. Minimum 6-month data requirement for model training")
print("  2. Baseline approach for insufficient data")
print("  3. Auto-ARIMA for optimal parameter selection")
print("  4. SARIMA for seasonal patterns")
print("  5. XGBoost with time-series features")
print("  6. Enhanced Prophet with custom regressors")
print("  7. Cross-validation for model selection")
print("  8. Weighted ensemble based on performance")
print("  9. Adaptive bounds and smoothing")
print(f"\nPrediction periods:")
print(f"  - August: {CURRENT_DAY} to 31 ({31 - CURRENT_DAY + 1} days)")
print(f"  - September: Full month (30 days)")
print(f"  - October: Full month (31 days)")
print(f"  - November: Full month (30 days)")

# Close database connection
conn_ps.close()