#!/usr/bin/env python3
"""
PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
ìë™/ìˆ˜ë™ ë°±ì—… ì‹¤í–‰ ë° ê´€ë¦¬
"""

import os
import sys
import subprocess
import gzip
import shutil
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Tuple
import json

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

from scripts.backup.backup_config import backup_config
from app.models.scheduler_log import SchedulerLog
from app.core.database import SessionLocal


class DatabaseBackup:
    """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í´ë˜ìŠ¤"""

    def __init__(self, config=None):
        self.config = config or backup_config
        self.setup_logging()

    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = self.config.get_log_filename("backup")

        logging.basicConfig(
            level=logging.INFO if self.config.verbose else logging.WARNING,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    def run_backup(self, backup_type: str = "manual", description: str = "") -> Tuple[bool, str]:
        """ë°±ì—… ì‹¤í–‰"""
        start_time = datetime.now()
        self.logger.info(f"{'='*60}")
        self.logger.info(f"ğŸš€ ë°±ì—… ì‹œì‘: {backup_type}")
        self.logger.info(f"ì‹œì‘ ì‹œê°„: {start_time}")
        self.logger.info(f"ë°ì´í„°ë² ì´ìŠ¤: {self.config.db_name}")
        if description:
            self.logger.info(f"ì„¤ëª…: {description}")

        try:
            # ë°±ì—… íŒŒì¼ëª… ìƒì„±
            backup_file = self.config.get_backup_filename(backup_type)
            self.logger.info(f"ë°±ì—… íŒŒì¼: {backup_file}")

            # pg_dump ëª…ë ¹ ìƒì„±
            cmd = self.config.get_pg_dump_command(backup_file)

            # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë¹„ë°€ë²ˆí˜¸)
            env = os.environ.copy()
            env['PGPASSWORD'] = self.config.db_password

            # ë°±ì—… ì‹¤í–‰
            self.logger.info(f"ëª…ë ¹ì–´: {' '.join(cmd[:-1])}...")  # ë¹„ë°€ë²ˆí˜¸ ì œì™¸
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                check=True
            )

            if result.stderr and self.config.verbose:
                self.logger.info(f"ë°±ì—… ì§„í–‰ ìƒí™©:\n{result.stderr}")

            # Plain SQL ì••ì¶• ì²˜ë¦¬
            if self.config.backup_format == "plain" and self.config.backup_compression:
                self.logger.info("íŒŒì¼ ì••ì¶• ì¤‘...")
                self._compress_file(backup_file)
                backup_file = f"{backup_file}.gz"

            # ë°±ì—… ê²€ì¦
            if self._verify_backup(backup_file):
                self.logger.info("âœ… ë°±ì—… ê²€ì¦ ì„±ê³µ")
            else:
                raise Exception("ë°±ì—… íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨")

            # ë°±ì—… ì •ë³´ ì €ì¥
            backup_info = self._save_backup_info(backup_file, backup_type, description, start_time)

            # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
            self._cleanup_old_backups(backup_type)

            # S3 ì—…ë¡œë“œ (ì„ íƒì )
            if self.config.s3_enabled:
                self._upload_to_s3(backup_file, backup_type)

            # ì„±ê³µ ë¡œê·¸
            duration = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"âœ… ë°±ì—… ì™„ë£Œ!")
            self.logger.info(f"ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
            self.logger.info(f"íŒŒì¼ í¬ê¸°: {self._get_file_size_str(backup_file)}")

            # ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ê¸°ë¡
            self._log_to_scheduler("database_backup", "success", backup_info)

            return True, backup_file

        except subprocess.CalledProcessError as e:
            self.logger.error(f"âŒ ë°±ì—… ì‹¤íŒ¨: {e}")
            self.logger.error(f"ì—ëŸ¬ ì¶œë ¥: {e.stderr}")
            self._log_to_scheduler("database_backup", "failed", {"error": str(e)})

            # ì´ë©”ì¼ ì•Œë¦¼
            if self.config.email_alerts:
                self._send_alert(f"ë°±ì—… ì‹¤íŒ¨: {e}")

            return False, str(e)

        except Exception as e:
            self.logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            self._log_to_scheduler("database_backup", "failed", {"error": str(e)})

            if self.config.email_alerts:
                self._send_alert(f"ë°±ì—… ì˜¤ë¥˜: {e}")

            return False, str(e)

        finally:
            self.logger.info(f"{'='*60}")

    def _compress_file(self, file_path: str):
        """íŒŒì¼ ì••ì¶•"""
        with open(file_path, 'rb') as f_in:
            with gzip.open(f"{file_path}.gz", 'wb', compresslevel=9) as f_out:
                shutil.copyfileobj(f_in, f_out)

        # ì›ë³¸ íŒŒì¼ ì‚­ì œ
        os.remove(file_path)
        self.logger.info(f"íŒŒì¼ ì••ì¶• ì™„ë£Œ: {file_path}.gz")

    def _verify_backup(self, backup_file: str) -> bool:
        """ë°±ì—… íŒŒì¼ ê²€ì¦"""
        if not Path(backup_file).exists():
            return False

        file_size = Path(backup_file).stat().st_size
        if file_size < 1024:  # 1KB ë¯¸ë§Œì´ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
            self.logger.warning(f"ë°±ì—… íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
            return False

        # Custom format ê²€ì¦
        if self.config.backup_format == "custom":
            try:
                # pg_restore -lë¡œ ëª©ì°¨ í™•ì¸
                cmd = ["pg_restore", "-l", backup_file]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
                return result.returncode == 0
            except Exception as e:
                self.logger.error(f"ë°±ì—… ê²€ì¦ ì‹¤íŒ¨: {e}")
                return False

        return True

    def _save_backup_info(self, backup_file: str, backup_type: str,
                         description: str, start_time: datetime) -> dict:
        """ë°±ì—… ì •ë³´ ì €ì¥"""
        info = {
            "file": backup_file,
            "type": backup_type,
            "description": description,
            "database": self.config.db_name,
            "timestamp": start_time.isoformat(),
            "size": Path(backup_file).stat().st_size,
            "format": self.config.backup_format,
            "compressed": self.config.backup_compression or backup_file.endswith('.gz'),
            "duration": (datetime.now() - start_time).total_seconds()
        }

        # JSON íŒŒì¼ë¡œ ë©”íƒ€ë°ì´í„° ì €ì¥
        info_file = f"{backup_file}.info.json"
        with open(info_file, 'w') as f:
            json.dump(info, f, indent=2)

        self.logger.info(f"ë°±ì—… ì •ë³´ ì €ì¥: {info_file}")
        return info

    def _cleanup_old_backups(self, backup_type: str):
        """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        backup_dir = self.config.backup_dir / backup_type
        cutoff_date = datetime.now() - timedelta(days=self.config.backup_retention_days)

        deleted_count = 0
        for file_path in backup_dir.iterdir():
            if file_path.is_file():
                # íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
                file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_time < cutoff_date:
                    self.logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {file_path.name}")
                    file_path.unlink()

                    # ë©”íƒ€ë°ì´í„° íŒŒì¼ë„ ì‚­ì œ
                    info_file = Path(f"{file_path}.info.json")
                    if info_file.exists():
                        info_file.unlink()

                    deleted_count += 1

        if deleted_count > 0:
            self.logger.info(f"{deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì‚­ì œë¨")

    def _upload_to_s3(self, backup_file: str, backup_type: str):
        """S3ì— ë°±ì—… íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            import boto3
            from botocore.exceptions import NoCredentialsError

            s3_client = boto3.client(
                's3',
                region_name=self.config.s3_region,
                aws_access_key_id=self.config.s3_access_key,
                aws_secret_access_key=self.config.s3_secret_key
            )

            file_name = Path(backup_file).name
            s3_key = f"{self.config.s3_prefix}/{backup_type}/{file_name}"

            self.logger.info(f"S3 ì—…ë¡œë“œ ì¤‘: {s3_key}")
            s3_client.upload_file(backup_file, self.config.s3_bucket, s3_key)
            self.logger.info(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{self.config.s3_bucket}/{s3_key}")

        except ImportError:
            self.logger.warning("boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. S3 ì—…ë¡œë“œ ê±´ë„ˆëœ€")
        except NoCredentialsError:
            self.logger.error("AWS ìê²© ì¦ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except Exception as e:
            self.logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _log_to_scheduler(self, job_name: str, status: str, result_summary: dict):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ê¸°ë¡"""
        try:
            db = SessionLocal()
            log = SchedulerLog(
                job_name=job_name,
                status=status,
                result_summary=json.dumps(result_summary),
                execution_time=datetime.now()
            )
            db.add(log)
            db.commit()
            db.close()
        except Exception as e:
            self.logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")

    def _send_alert(self, message: str):
        """ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""
        try:
            from app.services.email_service import EmailService

            email_service = EmailService()
            email_service.send_email(
                to_email=self.config.alert_email,
                subject="[PLAYAUTO] ë°±ì—… ì•Œë¦¼",
                body=f"""
                ë°±ì—… ì‘ì—… ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

                ì‹œê°„: {datetime.now()}
                ë°ì´í„°ë² ì´ìŠ¤: {self.config.db_name}
                ë©”ì‹œì§€: {message}

                ìì„¸í•œ ë‚´ìš©ì€ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.
                """
            )
            self.logger.info(f"ì•Œë¦¼ ì´ë©”ì¼ ì „ì†¡: {self.config.alert_email}")
        except Exception as e:
            self.logger.error(f"ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

    def _get_file_size_str(self, file_path: str) -> str:
        """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
        size = Path(file_path).stat().st_size
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} TB"

    def list_backups(self, backup_type: Optional[str] = None) -> list:
        """ë°±ì—… íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        backups = []

        if backup_type:
            dirs = [self.config.backup_dir / backup_type]
        else:
            dirs = [
                self.config.backup_dir / "daily",
                self.config.backup_dir / "weekly",
                self.config.backup_dir / "monthly",
                self.config.backup_dir / "manual"
            ]

        for backup_dir in dirs:
            if backup_dir.exists():
                for file_path in backup_dir.glob("*.dump*"):
                    if file_path.name.endswith('.info.json'):
                        continue

                    info_file = Path(f"{file_path}.info.json")
                    if info_file.exists():
                        with open(info_file, 'r') as f:
                            info = json.load(f)
                    else:
                        info = {
                            "file": str(file_path),
                            "type": backup_dir.name,
                            "timestamp": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                            "size": file_path.stat().st_size
                        }

                    backups.append(info)

        # ë‚ ì§œìˆœ ì •ë ¬
        backups.sort(key=lambda x: x['timestamp'], reverse=True)
        return backups


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…")
    parser.add_argument('--type', choices=['daily', 'weekly', 'monthly', 'manual'],
                       default='manual', help='ë°±ì—… íƒ€ì…')
    parser.add_argument('--description', help='ë°±ì—… ì„¤ëª…')
    parser.add_argument('--list', action='store_true', help='ë°±ì—… ëª©ë¡ ì¡°íšŒ')
    parser.add_argument('--schema-only', action='store_true', help='ìŠ¤í‚¤ë§ˆë§Œ ë°±ì—…')
    parser.add_argument('--data-only', action='store_true', help='ë°ì´í„°ë§Œ ë°±ì—…')

    args = parser.parse_args()

    backup = DatabaseBackup()

    if args.list:
        # ë°±ì—… ëª©ë¡ ì¶œë ¥
        backups = backup.list_backups()
        print(f"\nì´ {len(backups)}ê°œì˜ ë°±ì—… íŒŒì¼:")
        print("-" * 80)
        for b in backups:
            size_str = f"{b['size'] / (1024*1024):.2f} MB"
            print(f"[{b['type']}] {b['timestamp']} - {size_str} - {Path(b['file']).name}")
    else:
        # ë°±ì—… ì‹¤í–‰
        success, result = backup.run_backup(args.type, args.description or "")
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()